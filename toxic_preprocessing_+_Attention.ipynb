{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toxic_preprocessing + Attention",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iv75qyIvSiC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://arxiv.org/pdf/1805.12307.pdf 적용 예정.\n",
        "2020.04.21 -> preprocessing까지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJVY3KAY-eWV",
        "colab_type": "code",
        "outputId": "1cc194ec-aa76-4270-8bec-e2fb4e444f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4epwTeyCPDs5",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8V-9kCvSiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4GMpr5w-vmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/toxic/jigsaw-toxic-comment-classification-challenge')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIsNzzi6QB4p",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv.zip',compression='zip')\n",
        "test = pd.read_csv('test.csv.zip', compression='zip')\n",
        "test_label = pd.read_csv('test_labels.csv.zip',compression='zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5cCStTQYPhn",
        "colab": {}
      },
      "source": [
        "y = train.iloc[:,2:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2qMx7m4vSi3",
        "colab_type": "code",
        "outputId": "25796764-374f-49c7-9661-ccbeed54e86a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train.iloc[:,2:].sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "toxic            15294\n",
              "severe_toxic      1595\n",
              "obscene           8449\n",
              "threat             478\n",
              "insult            7877\n",
              "identity_hate     1405\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRd7Z7Euophz",
        "colab_type": "code",
        "outputId": "1d2a5aba-63e9-4e6c-ca57-3ae9ef453ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsnKhFCZvSi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import os, pickle, re\n",
        "from nltk.tokenize import word_tokenize  \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "stop_eng = stopwords.words('english')\n",
        "\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "class makeData:\n",
        "    def __init__(self, maxlen, embed_dim, minCount, mode='embedLayer'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "        self.minCount = minCount\n",
        "        if mode not in ['embedLayer','FastText']:\n",
        "            raise ValueError(\"Can not understand %s mode\"%(mode))\n",
        "        self.mode = mode\n",
        "        self.preprocessed_train_title = 'preprocessed_train_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.tokenized_train_title = 'tokenized_train_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.preprocessed_test_title = 'preprocessed_test_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.tokenized_test_title = 'tokenized_test_'+str(maxlen)+'_'+str(embed_dim)+'_'+str(minCount)+'.pkl'\n",
        "        self.word_list_name = 'word_list_'+mode+'_'+str(minCount)+'.pkl'\n",
        "        self.word_list = None\n",
        "    \n",
        "    def run(self,train,test):\n",
        "        df_train = self.fit(train,True)\n",
        "        df_test = self.fit(test,False)\n",
        "        return df_train, df_test\n",
        "    \n",
        "    def fit(self,df,is_train=False):#문장들만 넣어주세요\n",
        "        #전처리 끝난 데이터 있나요?\n",
        "        if self.preprocessed_train_title in os.listdir():\n",
        "            df =load_data(self.preprocessed_train_title)\n",
        "            return df\n",
        "        else:\n",
        "            #토큰화 끝난 데이터 있나요?\n",
        "            if self.tokenized_train_title in os.listdir():\n",
        "                df = load_data(self.tokenized_train_title)\n",
        "            else:\n",
        "                #tokenizer fit\n",
        "                if is_train == True:\n",
        "                    #토큰화 + 특수문자제거\n",
        "                    tokenizer = Tokenizer()\n",
        "                    tokenizer.fit_on_texts(df)\n",
        "                    #-------------------------\n",
        "                    #tokenizer는 dict 기반으로 단어->숫자 변환\n",
        "                    #word_index의 값을 삭제해 mincount, 불용어 처리\n",
        "                    #-------------------------\n",
        "                    #불용어 처리\n",
        "                    for i in stop_eng:\n",
        "                        tokenizer.word_index.pop(i,None)                    \n",
        "                    #minCount이하의 출현->삭제목록\n",
        "                    lessCount = dict(filter(lambda x:x[1]<5,tokenizer.word_counts.items()))\n",
        "                    for w in lessCount.keys():\n",
        "                        tokenizer.word_index.pop(w,None)\n",
        "                    self.tokenizer = tokenizer               \n",
        "                #test데이터는 train 데이터를 통해 토큰화 목록에 있는 친구들만 토큰화    \n",
        "                df=self.tokenizer.texts_to_sequences(df)\n",
        "            #토큰화 종료 -> len 맞춤\n",
        "            df = pad_sequences(df, maxlen=self.maxlen)\n",
        "            return df        \n",
        "    #이전버전\n",
        "    '''\n",
        "    def preprocess(self, df, is_train):\n",
        "        if is_train :\n",
        "        #모델에 임베딩 레이어 사용할 때\n",
        "            if self.word_list is None:\n",
        "                print('start to make valid word list')\n",
        "                if self.mode == 'embedLayer':\n",
        "                    counter = []\n",
        "                    for i in df:\n",
        "                        counter.extend(i)\n",
        "                    counter = Counter(counter)\n",
        "                    self.word_list = dict(list(filter(lambda x:x[1]>self.minCount, counter.items())))\n",
        "                    save_data(self.word_list_name,self.word_list)\n",
        "\n",
        "                #FastText로 임베딩된 워드 사용할 때\n",
        "                ## 0422 pretrainned Fast text 사용\n",
        "                elif self.mode == 'FastText':\n",
        "                    #name = 'fasttext_'+str(self.maxlen)+'_'+str(self.embed_dim)+'.vec'\n",
        "                    name = 'crawl-300d-2M.vec'\n",
        "                    if name in os.listdir():\n",
        "                        print(\"apply %s\"%(name))\n",
        "                        #self.ft_model = FastText.load(name)\n",
        "                        self.word_list = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('crawl-300d-2M.vec'))                    \n",
        "                    else:\n",
        "                        self.ft_model = FastText(train, size=embed_dim, window=5, min_count=self.minCount, workers=4, sg=1)\n",
        "                        self.ft_model.save(name)\n",
        "                        save_data(self.word_list_name,self.word_list)\n",
        "                        self.word_list = dict(self.ft_model.wv.vocab.items())\n",
        "        #----------\n",
        "        #word_list에 없는 단어들 버림\n",
        "        #1 epoch에 30분->10분 단축, accuracy 0.01상승\n",
        "        print('start to filter words')\n",
        "        for i, sentence in enumerate(df):\n",
        "            df[i] = list(filter(lambda x: x in self.word_list, sentence))\n",
        "        #이제는 mincount 이상의 빈도로 출현한 단어만 적용됨\n",
        "        #-----------\n",
        "        #문장의 단어 개수 통일        \n",
        "        print('start to fit shape')\n",
        "        if self.mode == 'embedLayer':\n",
        "            if is_train == True:\n",
        "                tokenizer = Tokenizer()\n",
        "                tokenizer.fit_on_texts(df)\n",
        "                self.el_tokenizer = tokenizer\n",
        "                \n",
        "                df = self.el_tokenizer.texts_to_sequences(df)\n",
        "                df = pad_sequences(df, maxlen=self.maxlen)\n",
        "                return df\n",
        "            else:                \n",
        "                df = self.el_tokenizer.texts_to_sequences(df)\n",
        "                df = pad_sequences(df, maxlen=self.maxlen)\n",
        "                return df\n",
        "        elif self.mode == 'FastText':\n",
        "            outlayer = np.zeros((len(df),self.maxlen,self.embed_dim))\n",
        "            for i, sentence in enumerate(tqdm_notebook(df)):\n",
        "                temp = np.zeros((self.maxlen,self.embed_dim))\n",
        "                index = self.maxlen-len(sentence) if len(sentence) < self.maxlen else 0\n",
        "                for word in sentence:\n",
        "                    if index == self.maxlen:\n",
        "                        break\n",
        "                    temp[index] = self.word_list[word]\n",
        "                    index+=1\n",
        "                outlayer[i] = temp\n",
        "            return outlayer        \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9clsjgJyPdpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.Model):\n",
        "\tdef __init__(self, units):\n",
        "\t\tsuper(Attention, self).__init__()\n",
        "\t\tself.W1 = tf.keras.layers.Dense(units)\n",
        "\t\tself.W2 = tf.keras.layers.Dense(units)\n",
        "\t\tself.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "\tdef call(self, features, hidden):\n",
        "\t\t#bidirectional LSTM 통해 나온 encoded sequence(x)와 hidden state(state_h)를 더해주기 위해 hidden_staet의 shape를 조절함\n",
        "\t\thidden_with_time_axis = tf.expand_dims(hidden, 1) #지금 단어 이전/이후 정보 있는 hidden state\n",
        "\t\t''' \n",
        "\t\t# score shape == (batch_size, max_length, 1)\n",
        "\t\t# we get 1 at the last axis because we are applying score to self.V\n",
        "\t\t# the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "\t\t'''\n",
        "\t\tscore = tf.nn.tanh(\n",
        "\t\t\tself.W1(features) + self.W2(hidden_with_time_axis)) #encoded sequence와 hidden state 더해주고 tanh로 activation\n",
        "\t\t# attention_weights shape == (batch_size, max_length, 1)\n",
        "\t\tattention_weights = tf.nn.softmax(self.V(score), axis=1) #softmax에 넘겨줘서 이를 0부터 1까지의 값을 가지는 attention score화\n",
        "\t\t  \n",
        "\t\t# context_vector shape after sum == (batch_size, hidden_size)\n",
        "\t\tcontext_vector = attention_weights * features #각각의 단어의 가중치(attention score)와 encoded sequences를 곱해줘서 context vector에 저장\n",
        "\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\t\treturn context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn-06MMi_aol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys,  re, csv, codecs\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "def predict_model_FastText(df_train,y,df_test):\n",
        "    inp = Input(shape=(maxlen,embed_dim, ))\n",
        "    x = LSTM(60, return_sequences=True,name='lstm_layer')(inp)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dense(200, activation=\"relu\")(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    batch_size = 256\n",
        "    epochs = 2\n",
        "    model.fit(df_train,y,batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "    model.save('model'+str(maxlen)+str(embed_dim)+'.h5')\n",
        "    t = model.predict(df_test, batch_size=batch_size)\n",
        "    sub = pd.read_csv('sample_submission.csv.zip',compression='zip')\n",
        "    subm = pd.DataFrame(t,columns=sub.columns[1:])\n",
        "    subm['id'] = sub['id']\n",
        "    subm.to_csv('sub4.csv',index=False)\n",
        "    return model\n",
        "    \n",
        "def predict_model(df_train,y,df_test):\n",
        "    max_feature = max(list(map(lambda x: max(x),df_train)))*2\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_feature,embed_dim)(inp)\n",
        "    x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer'))(x)\n",
        "    #Getting our Bidirectional LSTM output\n",
        "    (x, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(60, return_sequences=True, return_state=True))(x)\n",
        "    #forward, backward hidden state 합쳐주는 과정\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "    #Attention Layer 적용\n",
        "    context_vector, attention_weights = Attention(10)(x, state_h)\n",
        "    x = Dense(50, activation = 'relu')(context_vector) #context vector를 다시 dense로 펼쳐줌\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    batch_size = 32\n",
        "    epochs = 2\n",
        "    model.fit(df_train,y,batch_size=batch_size, epochs=epochs, validation_split=0.1,)\n",
        "    # model.save('model'+str(maxlen)+str(embed_dim)+'.h5')\n",
        "    t = model.predict(df_test, batch_size=batch_size)\n",
        "    sub = pd.read_csv('sample_submission.csv.zip',compression='zip')\n",
        "    subm = pd.DataFrame(t,columns=sub.columns[1:])\n",
        "    subm['id'] = sub['id']\n",
        "    \n",
        "    subm.to_csv('mySubmission.csv',index=False)\n",
        "    return model    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXN83gcQ_Ip8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen,embed_dim = 120, 200\n",
        "dataBuilder = makeData(maxlen, embed_dim, 5)\n",
        "\n",
        "df_train, df_test = dataBuilder.run(train['comment_text'],test['comment_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxECB9lxo279",
        "colab_type": "code",
        "outputId": "793c1183-2439-46fb-aab2-5056fbf17a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = predict_model(df_train,y,df_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4488/4488 [==============================] - 632s 141ms/step - loss: 0.0637 - accuracy: 0.9599 - val_loss: 0.0502 - val_accuracy: 0.9940\n",
            "Epoch 2/2\n",
            "4488/4488 [==============================] - 632s 141ms/step - loss: 0.0434 - accuracy: 0.9916 - val_loss: 0.0489 - val_accuracy: 0.9940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNp5BsKo44L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}